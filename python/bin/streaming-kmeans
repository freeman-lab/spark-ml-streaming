#!/usr/bin/env python

import time
import subprocess
import argparse
import os
import tempfile

from lightning import Lightning

from mlstreaming import StreamingDemo
from mlstreaming.util import findspark, findjar, baseargs

def main():

    # parse arguments
    parser = argparse.ArgumentParser(description='Streaming KMeans demo.')
    parser = baseargs(parser)
    parser.add_argument('-nc', '--ncenters', type=int, default=3, required=False, 
        help='Number of cluster centers')
    parser.add_argument('-nd', '--ndims', type=int, default=2, required=False, 
        help='Number of dimensions')
    parser.add_argument('-rs', '--randomseed', type=int, default=None, required=False,
        help='Random seed')
    parser.add_argument('-sd', '--std', type=float, default=0.3, required=False,
        help='Standard deviation of points')
    parser.add_argument('-up', '--update', type=str, choices=('jump', 'drift', 'none'), default='drift', required=False,
        help='Update behavior')
    parser.add_argument('-a', '--autoopen', type=bool, choices=(True, False), default=True, required=False,
        help='Whether to automatically open Lightning session on a browser')
    args = parser.parse_args()

    # basic setup
    sparkhome = findspark()
    jar = findjar()
    
    # set up lightning
    if args.lightning:
        lgn = Lightning(args.lightning)
        lgn.create_session('streaming-kmeans')
        if (args.autoopen):
            lgn.session.open()
    else:
        lgn = None

    # set temp path
    path = args.path
    if not path or path == '':
        path = tempfile.gettempdir()
    tmpdir = os.path.join(path, 'streamkmeans')

    # setup the demo
    s = StreamingDemo.make('kmeans', npoints=args.npoints, nbatches=args.nbatches)
    s.setup(tmpdir, overwrite=args.overwrite)
    s.params(ncenters=args.ncenters, ndims=args.ndims, std=args.std, seed=args.randomseed, update=args.update)

    # setup the spark job
    sparkSubmit = sparkhome + "/bin/spark-submit"
    sparkArgs = ["--class", "spark.mlstreaming.KMeans", jar]
    demoArgs = [s.datain, s.dataout, str(args.batchtime), str(args.ncenters), str(args.ndims), str(args.halflife), str(args.timeunit)]
    cmd = [sparkSubmit] + sparkArgs + demoArgs

    try:
        # start the spark job    
        p = subprocess.Popen(cmd)
        # wait for spark streaming to start up
        time.sleep(4)
        # start the demo
        s.run(lgn)

    finally:

       p.kill()

if __name__ == "__main__":
    main()

